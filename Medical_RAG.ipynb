{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abkimc/Medical-RAG./blob/main/Medical_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgonzDBUOIeU"
      },
      "outputs": [],
      "source": [
        " !pip install gradio transformers torch faiss-cpu sentence-transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOcemOWNi12J"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "import re\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# GPU MEMORY MANAGEMENT AND DEVICE SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"Setup device with proper error handling and memory management.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Clear GPU cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Check GPU memory\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"🔧 GPU detected: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)\")\n",
        "\n",
        "        # Set memory fraction to prevent OOM\n",
        "        torch.cuda.set_per_process_memory_fraction(0.8)\n",
        "\n",
        "        device = \"cuda\"\n",
        "        print(\"✅ Using CUDA device\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"⚠️ CUDA not available, using CPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data() -> pd.DataFrame:\n",
        "    \"\"\"Loads and preprocesses data from the Clalit URL.\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/abkimc/Scraping-Clalit-Webpages/refs/heads/main/Q%26A_Clalit.csv\"\n",
        "\n",
        "    try:\n",
        "        print(f\"🔄 Attempting to load data from URL: {url}\")\n",
        "        df = pd.read_csv(url, encoding='utf-8')\n",
        "        print(f\"✅ Successfully loaded data. Original columns: {df.columns.tolist()}\")\n",
        "        df = df.rename(columns={'question': 'question_he', 'answer': 'answer_he'})\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Fatal error loading data from URL: {e}\")\n",
        "        raise\n",
        "\n",
        "    if 'question_he' not in df.columns or 'answer_he' not in df.columns:\n",
        "        raise KeyError(\"Could not find required columns after renaming. Check CSV headers.\")\n",
        "\n",
        "    df['question_he'] = df['question_he'].astype(str).str.strip()\n",
        "    df['answer_he'] = df['answer_he'].astype(str).str.strip()\n",
        "    df = df.dropna(subset=['question_he', 'answer_he'])\n",
        "    df = df[df['question_he'] != '']\n",
        "    if 'subject' not in df.columns:\n",
        "        df['subject'] = 'כללי'\n",
        "    else:\n",
        "        df['subject'] = df['subject'].astype(str).str.strip().fillna('כללי')\n",
        "\n",
        "    print(f\"📊 Processed {len(df)} medical Q&A pairs.\")\n",
        "    return df\n",
        "\n",
        "def preprocess_hebrew_text(text: str) -> str:\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\u0590-\\u05FF\\s\\d\\.,!?;:\\-()]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def remove_hebrew_stop_words(text: str, stop_words: set) -> str:\n",
        "    \"\"\"Removes Hebrew stop words from a text.\"\"\"\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# ============================================================================\n",
        "# RAG COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class HebrewEmbeddingModel:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
        "        print(f\"🔄 Loading embedding model: {model_name}\")\n",
        "        self.device = setup_device()\n",
        "\n",
        "        try:\n",
        "            # Load model with explicit device handling\n",
        "            self.model = SentenceTransformer(model_name, device=self.device)\n",
        "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "            print(f\"✅ Embedding model loaded (dimension: {self.dimension}) on {self.device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ GPU loading failed, falling back to CPU: {e}\")\n",
        "            clear_gpu_memory()\n",
        "            self.device = \"cpu\"\n",
        "            self.model = SentenceTransformer(model_name, device=\"cpu\")\n",
        "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "            print(f\"✅ Embedding model loaded (dimension: {self.dimension}) on CPU\")\n",
        "\n",
        "    def encode(self, texts: List[str], show_progress: bool = True) -> np.ndarray:\n",
        "        try:\n",
        "            return self.model.encode(texts, show_progress_bar=show_progress, convert_to_numpy=True)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Encoding error on {self.device}, retrying on CPU: {e}\")\n",
        "            if self.device != \"cpu\":\n",
        "                clear_gpu_memory()\n",
        "                self.model = self.model.to(\"cpu\")\n",
        "                self.device = \"cpu\"\n",
        "            return self.model.encode(texts, show_progress_bar=show_progress, convert_to_numpy=True)\n",
        "\n",
        "class FAISSVectorStore:\n",
        "    def __init__(self, dimension: int):\n",
        "        self.dimension = dimension\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "\n",
        "    def add_embeddings(self, embeddings: np.ndarray):\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        print(f\"✅ Added {embeddings.shape[0]} embeddings to FAISS index\")\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        return self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "class HebrewLLM:\n",
        "    def __init__(self, model_name: str = \"google/medgemma-4b-it\", use_quantization: bool = True):\n",
        "        print(f\"🔄 Loading language model: {model_name}\")\n",
        "        self.model_name = model_name\n",
        "        self.device = setup_device()\n",
        "\n",
        "        # Determine if quantization should be used\n",
        "        use_quant = use_quantization and torch.cuda.is_available()\n",
        "\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        ) if use_quant else None\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model with error handling\n",
        "            if use_quant:\n",
        "                print(\"🔄 Loading model with 4-bit quantization...\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    quantization_config=quant_config,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.bfloat16\n",
        "                )\n",
        "            else:\n",
        "                print(\"🔄 Loading model without quantization...\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "                )\n",
        "\n",
        "            self.model.eval()\n",
        "            print(f\"✅ Language model loaded on {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model with GPU/quantization: {e}\")\n",
        "            print(\"🔄 Falling back to CPU without quantization...\")\n",
        "            clear_gpu_memory()\n",
        "\n",
        "            # Fallback to CPU\n",
        "            self.device = \"cpu\"\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                device_map=None,\n",
        "                torch_dtype=torch.float32,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            self.model.to(\"cpu\")\n",
        "            self.model.eval()\n",
        "            print(f\"✅ Language model loaded on CPU\")\n",
        "\n",
        "    def generate_response(self, prompt: str, max_length: int = 500) -> str:\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical assistant. You must answer in Hebrew.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            # Handle chat template safely\n",
        "            try:\n",
        "                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            except:\n",
        "                # Fallback if chat template not supported\n",
        "                full_prompt = f\"System: You are a helpful medical assistant. You must answer in Hebrew.\\nUser: {prompt}\\nAssistant:\"\n",
        "\n",
        "            # Tokenize with proper device handling\n",
        "            inputs = self.tokenizer.encode(\n",
        "                full_prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            )\n",
        "\n",
        "            # Move to correct device\n",
        "            if hasattr(self.model, 'device'):\n",
        "                inputs = inputs.to(self.model.device)\n",
        "            elif self.device == \"cuda\" and torch.cuda.is_available():\n",
        "                inputs = inputs.to(\"cuda\")\n",
        "            else:\n",
        "                inputs = inputs.to(\"cpu\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    attention_mask=torch.ones_like(inputs)\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Generation error: {e}\")\n",
        "            return \"מצטער, אירעה שגיאה ביצירת התשובה. אנא נסה שוב.\"\n",
        "\n",
        "class HebrewMedicalRAG:\n",
        "    def __init__(self):\n",
        "        print(\"🚀 Initializing Hebrew Medical RAG System...\")\n",
        "\n",
        "        # Clear memory before starting\n",
        "        clear_gpu_memory()\n",
        "\n",
        "        # Initialize components with error handling\n",
        "        self.df = load_and_preprocess_data()\n",
        "\n",
        "        print(\"🔄 Initializing embedding model...\")\n",
        "        self.embedding_model = HebrewEmbeddingModel()\n",
        "\n",
        "        print(\"🔄 Creating embeddings for retrieval...\")\n",
        "        self.embeddings = self.embedding_model.encode(self.df['question_he'].tolist())\n",
        "\n",
        "        print(\"🔄 Building vector store...\")\n",
        "        self.vector_store = FAISSVectorStore(self.embedding_model.dimension)\n",
        "        self.vector_store.add_embeddings(self.embeddings)\n",
        "\n",
        "        print(\"🔄 Initializing language model...\")\n",
        "        self.llm = HebrewLLM()\n",
        "\n",
        "        self.stop_words = set(['של', 'עם', 'את', 'על', 'כן', 'לא', 'אם', 'אני', 'אתה', 'הוא', 'היא', 'אנחנו', 'אתם', 'הם', 'הן', 'מה', 'מי', 'איך', 'כמה', 'למה', 'מתי', 'איפה', 'אבל', 'או', 'גם', 'ו', 'ב', 'ל', 'מ', 'ש'])\n",
        "\n",
        "        print(\"✅ RAG System initialized successfully!\")\n",
        "\n",
        "    def custom_distance_metric(self, query: str, retrieved_docs: List[Dict]) -> List[Dict]:\n",
        "        query_no_stopwords = remove_hebrew_stop_words(query, self.stop_words)\n",
        "        for doc in retrieved_docs:\n",
        "            semantic_score = doc['score']\n",
        "            doc_no_stopwords = remove_hebrew_stop_words(doc['question'], self.stop_words)\n",
        "            query_words = set(query_no_stopwords.split())\n",
        "            doc_words = set(doc_no_stopwords.split())\n",
        "            intersection = len(query_words.intersection(doc_words))\n",
        "            union = len(query_words.union(doc_words))\n",
        "            jaccard_score = intersection / union if union > 0 else 0\n",
        "            query_subject = \"כללי\"\n",
        "            doc_subject = doc.get('subject', 'כללי')\n",
        "            subject_score = 1.0 if query_subject.lower() == doc_subject.lower() else 0.5\n",
        "            doc['final_score'] = (0.6 * semantic_score) + (0.3 * jaccard_score) + (0.1 * subject_score)\n",
        "        return sorted(retrieved_docs, key=lambda x: x['final_score'], reverse=True)\n",
        "\n",
        "    def answer_question(self, query: str) -> Tuple[str, List[Dict]]:\n",
        "        try:\n",
        "            query = preprocess_hebrew_text(query)\n",
        "            if not query:\n",
        "                return \"אנא הקלד שאלה.\", []\n",
        "\n",
        "            query_embedding = self.embedding_model.encode([query], show_progress=False)\n",
        "            scores, indices = self.vector_store.search(query_embedding, top_k=10)\n",
        "\n",
        "            retrieved_contexts = [\n",
        "                {\n",
        "                    'question': self.df.iloc[idx]['question_he'],\n",
        "                    'answer': self.df.iloc[idx]['answer_he'],\n",
        "                    'subject': self.df.iloc[idx]['subject'],\n",
        "                    'score': float(scores[0][i])\n",
        "                }\n",
        "                for i, idx in enumerate(indices[0]) if scores[0][i] > 0.25\n",
        "            ]\n",
        "\n",
        "            if retrieved_contexts:\n",
        "                reranked_contexts = self.custom_distance_metric(query, retrieved_contexts)\n",
        "                contexts = reranked_contexts[:3]\n",
        "            else:\n",
        "                contexts = []\n",
        "\n",
        "            if not contexts:\n",
        "                return \"מצטער, לא מצאתי מידע רלוונטי במאגר לשאלתך. אנא נסה לנסח אותה מחדש או פנה לייעוץ רפואי.\", []\n",
        "\n",
        "            context_text = \"\\n\\n\".join([\n",
        "                f\"מידע רלוונטי {i+1}:\\n- נושא: {ctx['subject']}\\n- שאלה דומה: {ctx['question']}\\n- תשובה: {ctx['answer']}\"\n",
        "                for i, ctx in enumerate(contexts)\n",
        "            ])\n",
        "\n",
        "            prompt = f\"\"\"אתה עוזר רפואי וירטואלי, בעל ידע רב וגישה אמפתית. תפקידך הוא לענות על שאלות רפואיות בעברית.\n",
        "השתמש במידע הרלוונטי שסופק לך למטה כדי לבנות תשובה מקיפה, ברורה וקלה להבנה.\n",
        "אל תעתיק את התשובות מהמידע שסופק, אלא סנתז אותן לתשובה חדשה וקוהרנטית.\n",
        "פנה ישירות למשתמש והתייחס לשאלתו הספציפית.\n",
        "---\n",
        "[מידע רפואי זמין]\n",
        "{context_text}\n",
        "---\n",
        "[הנחיות]\n",
        "בהתבסס על המידע שלעיל, ענה על השאלה הבאה בצורה הטובה ביותר. היה ברור, אמפתי, והסבר את הדברים בפשטות.\n",
        "השאלה: \"{query}\"\n",
        "תשובה מנוסחת היטב:\"\"\"\n",
        "\n",
        "            answer = self.llm.generate_response(prompt)\n",
        "            return (answer if answer else contexts[0]['answer']), contexts\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error in answer_question: {e}\")\n",
        "            return f\"מצטער, אירעה שגיאה בעיבוד השאלה: {str(e)}\", []\n",
        "\n",
        "# ============================================================================\n",
        "# GRADIO INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "def create_gradio_interface(rag_system: HebrewMedicalRAG):\n",
        "    def chat_response(message: str, history: List = None) -> str:\n",
        "        if not message.strip():\n",
        "            return \"אנא הקלד שאלה.\"\n",
        "\n",
        "        try:\n",
        "            answer, contexts = rag_system.answer_question(message)\n",
        "            disclaimer = \"\\n\\n---\\n*⚠️ **הערה חשובה:** המידע כאן הוא למטרות לימוד בלבד ואינו מהווה תחליף לייעוץ רפואי מקצועי.*\"\n",
        "\n",
        "            if contexts:\n",
        "                sources = \"\\n\\n*מקורות מידע דומים שנמצאו (לפי רלוונטיות):*\\n\" + \"\\n\".join([\n",
        "                    f\"- *{ctx['question']} (רלוונטיות: {ctx.get('final_score', ctx['score']):.0%})*\"\n",
        "                    for ctx in contexts\n",
        "                ])\n",
        "                answer += sources\n",
        "\n",
        "            return answer + disclaimer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"מצטער, אירעה שגיאה: {str(e)}\"\n",
        "\n",
        "    return gr.Interface(\n",
        "        fn=chat_response,\n",
        "        inputs=gr.Textbox(\n",
        "            label=\"שאל שאלה רפואית בעברית\",\n",
        "            placeholder=\"לדוגמה: מה הטיפול בפטרת ציפורניים?\",\n",
        "            lines=2\n",
        "        ),\n",
        "        outputs=gr.Markdown(label=\"תשובה\"),\n",
        "        title=\"🏥 עוזר רפואי דיגיטלי (מבוסס על נתוני כללית)\",\n",
        "        description=\"שאל אותי שאלה רפואית בעברית, ואנסה לענות בהתבסס על מאגר המידע. **אזהרה:** איני רופא. המידע הוא למטרות לימוד בלבד.\",\n",
        "        examples=[\n",
        "            [\"מה הטיפול בפטרת ציפורניים?\"],\n",
        "            [\"מהי אנמיה?\"],\n",
        "            [\"מתי יש צורך בבדיקת סיטי?\"]\n",
        "        ],\n",
        "        theme='soft',\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    try:\n",
        "        # Authentication\n",
        "        print(\"🔑 Logging into Hugging Face Hub...\")\n",
        "        token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "        if token:\n",
        "            print(\"✅ Found Hugging Face token in environment variables.\")\n",
        "            login(token=token)\n",
        "        else:\n",
        "            print(\"⚠️ No token found in environment. You may need to log in interactively.\")\n",
        "            try:\n",
        "                login()\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Login failed: {e}. Continuing without login (may affect model access).\")\n",
        "\n",
        "        print(\"✅ Authentication complete.\")\n",
        "\n",
        "        # Setup environment variables for CUDA debugging\n",
        "        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "        print(f\"🔧 Gradio version: {gr.__version__}\")\n",
        "        print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
        "        print(f\"🔧 CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"🔧 CUDA version: {torch.version.cuda}\")\n",
        "            print(f\"🔧 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # Initialize RAG system\n",
        "        rag_system = HebrewMedicalRAG()\n",
        "\n",
        "        # Create and launch interface\n",
        "        interface = create_gradio_interface(rag_system)\n",
        "        print(\"\\n🎉 Launching Hebrew Medical RAG Chatbot! A public URL will be generated shortly.\")\n",
        "        interface.launch(share=True, debug=False, show_error=True, quiet=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ A critical error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Try to free GPU memory\n",
        "        clear_gpu_memory()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtyouQRkfAthWhDncUUsYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}