{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abkimc/Medical-RAG./blob/main/Medical_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgonzDBUOIeU"
      },
      "outputs": [],
      "source": [
        " !pip install gradio transformers torch faiss-cpu sentence-transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOcemOWNi12J"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "import re\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# GPU MEMORY MANAGEMENT AND DEVICE SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"Setup device with proper error handling and memory management.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Clear GPU cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Check GPU memory\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"ğŸ”§ GPU detected: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)\")\n",
        "\n",
        "        # Set memory fraction to prevent OOM\n",
        "        torch.cuda.set_per_process_memory_fraction(0.8)\n",
        "\n",
        "        device = \"cuda\"\n",
        "        print(\"âœ… Using CUDA device\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"âš ï¸ CUDA not available, using CPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data() -> pd.DataFrame:\n",
        "    \"\"\"Loads and preprocesses data from the Clalit URL.\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/abkimc/Scraping-Clalit-Webpages/refs/heads/main/Q%26A_Clalit.csv\"\n",
        "\n",
        "    try:\n",
        "        print(f\"ğŸ”„ Attempting to load data from URL: {url}\")\n",
        "        df = pd.read_csv(url, encoding='utf-8')\n",
        "        print(f\"âœ… Successfully loaded data. Original columns: {df.columns.tolist()}\")\n",
        "        df = df.rename(columns={'question': 'question_he', 'answer': 'answer_he'})\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Fatal error loading data from URL: {e}\")\n",
        "        raise\n",
        "\n",
        "    if 'question_he' not in df.columns or 'answer_he' not in df.columns:\n",
        "        raise KeyError(\"Could not find required columns after renaming. Check CSV headers.\")\n",
        "\n",
        "    df['question_he'] = df['question_he'].astype(str).str.strip()\n",
        "    df['answer_he'] = df['answer_he'].astype(str).str.strip()\n",
        "    df = df.dropna(subset=['question_he', 'answer_he'])\n",
        "    df = df[df['question_he'] != '']\n",
        "    if 'subject' not in df.columns:\n",
        "        df['subject'] = '×›×œ×œ×™'\n",
        "    else:\n",
        "        df['subject'] = df['subject'].astype(str).str.strip().fillna('×›×œ×œ×™')\n",
        "\n",
        "    print(f\"ğŸ“Š Processed {len(df)} medical Q&A pairs.\")\n",
        "    return df\n",
        "\n",
        "def preprocess_hebrew_text(text: str) -> str:\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\u0590-\\u05FF\\s\\d\\.,!?;:\\-()]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def remove_hebrew_stop_words(text: str, stop_words: set) -> str:\n",
        "    \"\"\"Removes Hebrew stop words from a text.\"\"\"\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# ============================================================================\n",
        "# RAG COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class HebrewEmbeddingModel:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
        "        print(f\"ğŸ”„ Loading embedding model: {model_name}\")\n",
        "        self.device = setup_device()\n",
        "\n",
        "        try:\n",
        "            # Load model with explicit device handling\n",
        "            self.model = SentenceTransformer(model_name, device=self.device)\n",
        "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "            print(f\"âœ… Embedding model loaded (dimension: {self.dimension}) on {self.device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ GPU loading failed, falling back to CPU: {e}\")\n",
        "            clear_gpu_memory()\n",
        "            self.device = \"cpu\"\n",
        "            self.model = SentenceTransformer(model_name, device=\"cpu\")\n",
        "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "            print(f\"âœ… Embedding model loaded (dimension: {self.dimension}) on CPU\")\n",
        "\n",
        "    def encode(self, texts: List[str], show_progress: bool = True) -> np.ndarray:\n",
        "        try:\n",
        "            return self.model.encode(texts, show_progress_bar=show_progress, convert_to_numpy=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Encoding error on {self.device}, retrying on CPU: {e}\")\n",
        "            if self.device != \"cpu\":\n",
        "                clear_gpu_memory()\n",
        "                self.model = self.model.to(\"cpu\")\n",
        "                self.device = \"cpu\"\n",
        "            return self.model.encode(texts, show_progress_bar=show_progress, convert_to_numpy=True)\n",
        "\n",
        "class FAISSVectorStore:\n",
        "    def __init__(self, dimension: int):\n",
        "        self.dimension = dimension\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "\n",
        "    def add_embeddings(self, embeddings: np.ndarray):\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        print(f\"âœ… Added {embeddings.shape[0]} embeddings to FAISS index\")\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        return self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "class HebrewLLM:\n",
        "    def __init__(self, model_name: str = \"google/medgemma-4b-it\", use_quantization: bool = True):\n",
        "        print(f\"ğŸ”„ Loading language model: {model_name}\")\n",
        "        self.model_name = model_name\n",
        "        self.device = setup_device()\n",
        "\n",
        "        # Determine if quantization should be used\n",
        "        use_quant = use_quantization and torch.cuda.is_available()\n",
        "\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        ) if use_quant else None\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model with error handling\n",
        "            if use_quant:\n",
        "                print(\"ğŸ”„ Loading model with 4-bit quantization...\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    quantization_config=quant_config,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.bfloat16\n",
        "                )\n",
        "            else:\n",
        "                print(\"ğŸ”„ Loading model without quantization...\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "                )\n",
        "\n",
        "            self.model.eval()\n",
        "            print(f\"âœ… Language model loaded on {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading model with GPU/quantization: {e}\")\n",
        "            print(\"ğŸ”„ Falling back to CPU without quantization...\")\n",
        "            clear_gpu_memory()\n",
        "\n",
        "            # Fallback to CPU\n",
        "            self.device = \"cpu\"\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                device_map=None,\n",
        "                torch_dtype=torch.float32,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            self.model.to(\"cpu\")\n",
        "            self.model.eval()\n",
        "            print(f\"âœ… Language model loaded on CPU\")\n",
        "\n",
        "    def generate_response(self, prompt: str, max_length: int = 500) -> str:\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical assistant. You must answer in Hebrew.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            # Handle chat template safely\n",
        "            try:\n",
        "                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            except:\n",
        "                # Fallback if chat template not supported\n",
        "                full_prompt = f\"System: You are a helpful medical assistant. You must answer in Hebrew.\\nUser: {prompt}\\nAssistant:\"\n",
        "\n",
        "            # Tokenize with proper device handling\n",
        "            inputs = self.tokenizer.encode(\n",
        "                full_prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            )\n",
        "\n",
        "            # Move to correct device\n",
        "            if hasattr(self.model, 'device'):\n",
        "                inputs = inputs.to(self.model.device)\n",
        "            elif self.device == \"cuda\" and torch.cuda.is_available():\n",
        "                inputs = inputs.to(\"cuda\")\n",
        "            else:\n",
        "                inputs = inputs.to(\"cpu\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    attention_mask=torch.ones_like(inputs)\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Generation error: {e}\")\n",
        "            return \"××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ×‘×™×¦×™×¨×ª ×”×ª×©×•×‘×”. ×× × × ×¡×” ×©×•×‘.\"\n",
        "\n",
        "class HebrewMedicalRAG:\n",
        "    def __init__(self):\n",
        "        print(\"ğŸš€ Initializing Hebrew Medical RAG System...\")\n",
        "\n",
        "        # Clear memory before starting\n",
        "        clear_gpu_memory()\n",
        "\n",
        "        # Initialize components with error handling\n",
        "        self.df = load_and_preprocess_data()\n",
        "\n",
        "        print(\"ğŸ”„ Initializing embedding model...\")\n",
        "        self.embedding_model = HebrewEmbeddingModel()\n",
        "\n",
        "        print(\"ğŸ”„ Creating embeddings for retrieval...\")\n",
        "        self.embeddings = self.embedding_model.encode(self.df['question_he'].tolist())\n",
        "\n",
        "        print(\"ğŸ”„ Building vector store...\")\n",
        "        self.vector_store = FAISSVectorStore(self.embedding_model.dimension)\n",
        "        self.vector_store.add_embeddings(self.embeddings)\n",
        "\n",
        "        print(\"ğŸ”„ Initializing language model...\")\n",
        "        self.llm = HebrewLLM()\n",
        "\n",
        "        self.stop_words = set(['×©×œ', '×¢×', '××ª', '×¢×œ', '×›×Ÿ', '×œ×', '××', '×× ×™', '××ª×”', '×”×•×', '×”×™×', '×× ×—× ×•', '××ª×', '×”×', '×”×Ÿ', '××”', '××™', '××™×š', '×›××”', '×œ××”', '××ª×™', '××™×¤×”', '××‘×œ', '××•', '×’×', '×•', '×‘', '×œ', '×', '×©'])\n",
        "\n",
        "        print(\"âœ… RAG System initialized successfully!\")\n",
        "\n",
        "    def custom_distance_metric(self, query: str, retrieved_docs: List[Dict]) -> List[Dict]:\n",
        "        query_no_stopwords = remove_hebrew_stop_words(query, self.stop_words)\n",
        "        for doc in retrieved_docs:\n",
        "            semantic_score = doc['score']\n",
        "            doc_no_stopwords = remove_hebrew_stop_words(doc['question'], self.stop_words)\n",
        "            query_words = set(query_no_stopwords.split())\n",
        "            doc_words = set(doc_no_stopwords.split())\n",
        "            intersection = len(query_words.intersection(doc_words))\n",
        "            union = len(query_words.union(doc_words))\n",
        "            jaccard_score = intersection / union if union > 0 else 0\n",
        "            query_subject = \"×›×œ×œ×™\"\n",
        "            doc_subject = doc.get('subject', '×›×œ×œ×™')\n",
        "            subject_score = 1.0 if query_subject.lower() == doc_subject.lower() else 0.5\n",
        "            doc['final_score'] = (0.6 * semantic_score) + (0.3 * jaccard_score) + (0.1 * subject_score)\n",
        "        return sorted(retrieved_docs, key=lambda x: x['final_score'], reverse=True)\n",
        "\n",
        "    def answer_question(self, query: str) -> Tuple[str, List[Dict]]:\n",
        "        try:\n",
        "            query = preprocess_hebrew_text(query)\n",
        "            if not query:\n",
        "                return \"×× × ×”×§×œ×“ ×©××œ×”.\", []\n",
        "\n",
        "            query_embedding = self.embedding_model.encode([query], show_progress=False)\n",
        "            scores, indices = self.vector_store.search(query_embedding, top_k=10)\n",
        "\n",
        "            retrieved_contexts = [\n",
        "                {\n",
        "                    'question': self.df.iloc[idx]['question_he'],\n",
        "                    'answer': self.df.iloc[idx]['answer_he'],\n",
        "                    'subject': self.df.iloc[idx]['subject'],\n",
        "                    'score': float(scores[0][i])\n",
        "                }\n",
        "                for i, idx in enumerate(indices[0]) if scores[0][i] > 0.25\n",
        "            ]\n",
        "\n",
        "            if retrieved_contexts:\n",
        "                reranked_contexts = self.custom_distance_metric(query, retrieved_contexts)\n",
        "                contexts = reranked_contexts[:3]\n",
        "            else:\n",
        "                contexts = []\n",
        "\n",
        "            if not contexts:\n",
        "                return \"××¦×˜×¢×¨, ×œ× ××¦××ª×™ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘×××’×¨ ×œ×©××œ×ª×š. ×× × × ×¡×” ×œ× ×¡×— ××•×ª×” ××—×“×© ××• ×¤× ×” ×œ×™×™×¢×•×¥ ×¨×¤×•××™.\", []\n",
        "\n",
        "            context_text = \"\\n\\n\".join([\n",
        "                f\"××™×“×¢ ×¨×œ×•×•× ×˜×™ {i+1}:\\n- × ×•×©×: {ctx['subject']}\\n- ×©××œ×” ×“×•××”: {ctx['question']}\\n- ×ª×©×•×‘×”: {ctx['answer']}\"\n",
        "                for i, ctx in enumerate(contexts)\n",
        "            ])\n",
        "\n",
        "            prompt = f\"\"\"××ª×” ×¢×•×–×¨ ×¨×¤×•××™ ×•×™×¨×˜×•××œ×™, ×‘×¢×œ ×™×“×¢ ×¨×‘ ×•×’×™×©×” ×××¤×ª×™×ª. ×ª×¤×§×™×“×š ×”×•× ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª ×¨×¤×•××™×•×ª ×‘×¢×‘×¨×™×ª.\n",
        "×”×©×ª××© ×‘××™×“×¢ ×”×¨×œ×•×•× ×˜×™ ×©×¡×•×¤×§ ×œ×š ×œ××˜×” ×›×“×™ ×œ×‘× ×•×ª ×ª×©×•×‘×” ××§×™×¤×”, ×‘×¨×•×¨×” ×•×§×œ×” ×œ×”×‘× ×”.\n",
        "××œ ×ª×¢×ª×™×§ ××ª ×”×ª×©×•×‘×•×ª ××”××™×“×¢ ×©×¡×•×¤×§, ××œ× ×¡× ×ª×– ××•×ª×Ÿ ×œ×ª×©×•×‘×” ×—×“×©×” ×•×§×•×”×¨× ×˜×™×ª.\n",
        "×¤× ×” ×™×©×™×¨×•×ª ×œ××©×ª××© ×•×”×ª×™×™×—×¡ ×œ×©××œ×ª×• ×”×¡×¤×¦×™×¤×™×ª.\n",
        "---\n",
        "[××™×“×¢ ×¨×¤×•××™ ×–××™×Ÿ]\n",
        "{context_text}\n",
        "---\n",
        "[×”× ×—×™×•×ª]\n",
        "×‘×”×ª×‘×¡×¡ ×¢×œ ×”××™×“×¢ ×©×œ×¢×™×œ, ×¢× ×” ×¢×œ ×”×©××œ×” ×”×‘××” ×‘×¦×•×¨×” ×”×˜×•×‘×” ×‘×™×•×ª×¨. ×”×™×” ×‘×¨×•×¨, ×××¤×ª×™, ×•×”×¡×‘×¨ ××ª ×”×“×‘×¨×™× ×‘×¤×©×˜×•×ª.\n",
        "×”×©××œ×”: \"{query}\"\n",
        "×ª×©×•×‘×” ×× ×•×¡×—×ª ×”×™×˜×‘:\"\"\"\n",
        "\n",
        "            answer = self.llm.generate_response(prompt)\n",
        "            return (answer if answer else contexts[0]['answer']), contexts\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error in answer_question: {e}\")\n",
        "            return f\"××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ×‘×¢×™×‘×•×“ ×”×©××œ×”: {str(e)}\", []\n",
        "\n",
        "# ============================================================================\n",
        "# GRADIO INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "def create_gradio_interface(rag_system: HebrewMedicalRAG):\n",
        "    def chat_response(message: str, history: List = None) -> str:\n",
        "        if not message.strip():\n",
        "            return \"×× × ×”×§×œ×“ ×©××œ×”.\"\n",
        "\n",
        "        try:\n",
        "            answer, contexts = rag_system.answer_question(message)\n",
        "            disclaimer = \"\\n\\n---\\n*âš ï¸ **×”×¢×¨×” ×—×©×•×‘×”:** ×”××™×“×¢ ×›××Ÿ ×”×•× ×œ××˜×¨×•×ª ×œ×™××•×“ ×‘×œ×‘×“ ×•××™× ×• ××”×•×•×” ×ª×—×œ×™×£ ×œ×™×™×¢×•×¥ ×¨×¤×•××™ ××§×¦×•×¢×™.*\"\n",
        "\n",
        "            if contexts:\n",
        "                sources = \"\\n\\n*××§×•×¨×•×ª ××™×“×¢ ×“×•××™× ×©× ××¦××• (×œ×¤×™ ×¨×œ×•×•× ×˜×™×•×ª):*\\n\" + \"\\n\".join([\n",
        "                    f\"- *{ctx['question']} (×¨×œ×•×•× ×˜×™×•×ª: {ctx.get('final_score', ctx['score']):.0%})*\"\n",
        "                    for ctx in contexts\n",
        "                ])\n",
        "                answer += sources\n",
        "\n",
        "            return answer + disclaimer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××”: {str(e)}\"\n",
        "\n",
        "    return gr.Interface(\n",
        "        fn=chat_response,\n",
        "        inputs=gr.Textbox(\n",
        "            label=\"×©××œ ×©××œ×” ×¨×¤×•××™×ª ×‘×¢×‘×¨×™×ª\",\n",
        "            placeholder=\"×œ×“×•×’××”: ××” ×”×˜×™×¤×•×œ ×‘×¤×˜×¨×ª ×¦×™×¤×•×¨× ×™×™×?\",\n",
        "            lines=2\n",
        "        ),\n",
        "        outputs=gr.Markdown(label=\"×ª×©×•×‘×”\"),\n",
        "        title=\"ğŸ¥ ×¢×•×–×¨ ×¨×¤×•××™ ×“×™×’×™×˜×œ×™ (××‘×•×¡×¡ ×¢×œ × ×ª×•× ×™ ×›×œ×œ×™×ª)\",\n",
        "        description=\"×©××œ ××•×ª×™ ×©××œ×” ×¨×¤×•××™×ª ×‘×¢×‘×¨×™×ª, ×•×× ×¡×” ×œ×¢× ×•×ª ×‘×”×ª×‘×¡×¡ ×¢×œ ×××’×¨ ×”××™×“×¢. **××–×”×¨×”:** ××™× ×™ ×¨×•×¤×. ×”××™×“×¢ ×”×•× ×œ××˜×¨×•×ª ×œ×™××•×“ ×‘×œ×‘×“.\",\n",
        "        examples=[\n",
        "            [\"××” ×”×˜×™×¤×•×œ ×‘×¤×˜×¨×ª ×¦×™×¤×•×¨× ×™×™×?\"],\n",
        "            [\"××”×™ ×× ××™×”?\"],\n",
        "            [\"××ª×™ ×™×© ×¦×•×¨×š ×‘×‘×“×™×§×ª ×¡×™×˜×™?\"]\n",
        "        ],\n",
        "        theme='soft',\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    try:\n",
        "        # Authentication\n",
        "        print(\"ğŸ”‘ Logging into Hugging Face Hub...\")\n",
        "        token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "        if token:\n",
        "            print(\"âœ… Found Hugging Face token in environment variables.\")\n",
        "            login(token=token)\n",
        "        else:\n",
        "            print(\"âš ï¸ No token found in environment. You may need to log in interactively.\")\n",
        "            try:\n",
        "                login()\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Login failed: {e}. Continuing without login (may affect model access).\")\n",
        "\n",
        "        print(\"âœ… Authentication complete.\")\n",
        "\n",
        "        # Setup environment variables for CUDA debugging\n",
        "        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "        print(f\"ğŸ”§ Gradio version: {gr.__version__}\")\n",
        "        print(f\"ğŸ”§ PyTorch version: {torch.__version__}\")\n",
        "        print(f\"ğŸ”§ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"ğŸ”§ CUDA version: {torch.version.cuda}\")\n",
        "            print(f\"ğŸ”§ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # Initialize RAG system\n",
        "        rag_system = HebrewMedicalRAG()\n",
        "\n",
        "        # Create and launch interface\n",
        "        interface = create_gradio_interface(rag_system)\n",
        "        print(\"\\nğŸ‰ Launching Hebrew Medical RAG Chatbot! A public URL will be generated shortly.\")\n",
        "        interface.launch(share=True, debug=False, show_error=True, quiet=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ A critical error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Try to free GPU memory\n",
        "        clear_gpu_memory()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtyouQRkfAthWhDncUUsYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}